return(density_function)
}
# calculate the \hat(pi_i) = \pi(Z_i) \prod(p(X_j | Z+i))
# calculate pi_i
# prior setting
# designate sampling function
density_function = density_from_dist(d, "uniform", min = 0, max = 10)
pi_z = density_function(res_points)
pi_z
# conditioning X ftn for vectoization
z_conditioning_x = function(z) { return(prob_density(X, z)) }
pi_i = exp(log(pi_z) + colSums(log(apply(res_points, 1, z_conditioning_x))))
omega_i = pi_i / rho_i
omega_i
pi_i
colSums(log(apply(res_points, 1, z_conditioning_x)))
res_points
colSums(log(apply(res_points, 1, z_conditioning_x)))
log(pi_z
)
log(apply(res_points, 1, z_conditioning_x)))
log(apply(res_points, 1, z_conditioning_x))
res_points
apply(res_points, 1, z_conditioning_x)
res_points
apply(res_points, 2, z_conditioning_x)
apply(res_points, 1, z_conditioning_x)
res_point
res_points
prob_density = function(x, theta) {return(dnorm(x, mean = theta[[1]], sd = theta[[2]]))}
z_conditioning_x = function(z) { return(prob_density(X, z))}
z_conditioning_x(res_points[1, ])
log(apply(res_points, 1, z_conditioning_x)))
colSums(log(apply(res_points, 1, z_conditioning_x)))
X
pi_i = exp(log(pi_z) + colSums(log(apply(res_points, 1, z_conditioning_x))))
omega_i = pi_i / rho_i
omega_i
X
plot(x = seq(1:nrow(theta_updated)), y = apply(theta_updated, 1, C), xlab = "step", ylab = "negative log likelihood", type = 'o')
head(res_points)
phi(res_points[1, ]) # are they on solution manifold?
plot(res_points, xlab = "mean", ylab = "sigma", type = 'o')
# create quadratic function from given phi and Lambda
pd_function = function(phi, Lambda) {
quadratic_f = function(x) {
return(as.numeric(crossprod(phi(x), Lambda %*% phi(x))))
}
return(quadratic_f)
}
# calculate the gradient of quadratic function
grad_of_quadratic_f = function(phi, Lambda) {
grad_f = function(x) {
return(crossprod(2 * phi(x), Lambda %*% rootSolve::gradient(phi, x)))
}
return(grad_f)
}
# calculate the gradient of general function.
grad_of_f = function(f) {
grad_f = function(x) {
return(rootSolve::gradient(f, x))
}
return(grad_f)
}
# measure distance between rows of two matrices.
between_row_dist = function(X, M) {
mat_dist = outer(rowSums(X^2), rowSums(M^2), '+') - tcrossprod(X, 2 * M)
return(mat_dist)
}
# sampling from given prior. Also conduct compatibility check.
sampling_from_dist = function(d, prior, ...) {
params = list(...)
if(prior == "gaussian") {
# set mu
if(exists('mean', params)) {
mu = params$mean
# compatibility check
if(length(mean) != d) {
stop("dimension of mean mismatches with d.")
}
} else{ # default
print("No mean declaration. Set to default 0.")
mu = rep(0, d)
}
# set sigma
if(exists('sigma', params)) {
sigma = params$sigma
# compatibility check
if((nrow(sigma) != d) || (ncol(sigma) != d)) {
stop("dimension of sigma mismatches with d.")
}
if(!matrixcalc::is.positive.definite(sigma)) {
stop("sigma must be positive definite.")
}
} else{ # default
print("No sigma declaration. Set to default identity matrix.")
sigma = diag(d)
}
# sampling function
sampling_function = function(N) {
(mvtnorm::rmvnorm(N, mean = mean, sigma = sigma))
}
} else if (prior == "uniform") { # set parameter for uniform distribution
# set lower
if(exists('min', params)) {
lower = params$min
if(!is.numeric(lower)) {
stop("min must be numeric.")
}
} else {
print("No min declaration. Set to default 0.")
lower = 0
}
# set upper
if(exists('max', params)) {
upper = params$max
if(!is.numeric(upper)) {
stop("max must be numeric.")
}
} else {
print("No max declaration. Set to default 1")
upper = 1
}
# compatibility check
if(lower >= upper) {
stop("min must be smaller than max.")
}
# sampling function
sampling_function = function(N) {
return(matrix(stats::runif(N * d, min = lower, max = upper), N, d))
}
} else{
stop("Not implemented yet. Please use gaussian or uniform as a prior.")
}
return(sampling_function)
}
# density from given prior. Also conduct compatibility check.
density_from_dist = function(d, prior, ...) {
params = list(...)
if(prior == "gaussian") {
# set mu
if(exists('mean', params)) {
mu = params$mean
# compatibility check
if(length(mu) != d) {
stop("dimension of mean mismatches with d.")
}
} else{ # default
print("No mean declaration. Set to default 0.")
mu = rep(0, d)
}
# set sigma
if(exists('sigma', params)) {
sigma = params$sigma
# compatibility check
if((nrow(sigma) != d) || (ncol(sigma) != d)) {
stop("dimension of sigma mismatches with d.")
}
if(!matrixcalc::is.positive.definite(sigma)) {
stop("sigma must be positive definite.")
}
} else{ # default
print("No sigma declaration. Set to default identity matrix.")
sigma = diag(d)
}
# density function
density_function = function(x) {
(mvtnorm::dmvnorm(x, mean = mean, sigma = sigma))
}
} else if (prior == "uniform") { # set parameter for uniform distribution
# set lower
if(exists('min', params)) {
lower = params$min
if(!is.numeric(lower)) {
stop("min must be numeric.")
}
} else {
print("No min declaration. Set to default 0.")
lower = 0
}
# set upper
if(exists('max', params)) {
upper = params$max
if(!is.numeric(upper)) {
stop("max must be numeric.")
}
} else {
print("No max declaration. Set to default 1")
upper = 1
}
# compatibility check
if(lower >= upper) {
stop("min must be smaller than max.")
}
# density function
density_function = function(x) {
each_density = stats::dunif(x, min = lower, max = upper)
return(apply(each_density, 1, prod))
}
} else{
stop("Not implemented yet. Please use gaussian or uniform as a prior.")
}
return(density_function)
}
# gradient descent in solution manifold.
sol_mfd_grad_descent = function(N, d, quadratic_f, grad_f, sampling_function, gamma = 0.1, tol1 = 1e-07, tol2 = 1e-15, num_iter = 1000) {
final_mat = matrix(0, N, d)
i = 1
iter = 0
while(i <= N) {
# sampling required points
curr_point = sampling_function(1)
curr_point = grad_descent(curr_point, grad_f, d, gamma, tol2, num_iter)
# whether obtained points are in sol_mfd
eval_point = as.numeric(quadratic_f(curr_point))
# no convergence to manifold.
if(eval_point > tol1) {
next
}
# conv to manifold
final_mat[i, ] = curr_point
# updated number of samples this time
i = i + 1
# reached maximum number of iteration
if (num_iter < iter) {
stop("Reached maximum iteration before getting N points. Try larger num_iter or different hyperparams.")
}
iter = iter + 1
}
return(final_mat)
}
# gradient descending in general case.
grad_descent = function(curr_point, grad_f, d, gamma, tol, num_iter) {
# error: to evaluate the convergence of grad_descent
error = sum((curr_point)^2)
new_point = curr_point
iter = 0
# perform grad_descent
while(error > tol) {
# update
new_point = curr_point - gamma * grad_f(curr_point)
# checking convergence
error = sum((new_point - curr_point)^2)
# update
curr_point = new_point
# check maximum iter
if(iter > num_iter) {
stop("Reached maximum iteration before grad_descent converge. Try larger num_iter or different hyperparams.")
}
iter = iter + 1
}
return(curr_point)
}
#' solution manifold points sampling
#'
#' @param N int: number of output data
#' @param phi target function: R^d -> R^s
#' @param d int: input dimension
#' @param prior str: type of prior distribution. "Gaussian", "t" are currently provided
#' @param s int: output dimension
#' @param gamma double: parameter of gradient descent step size
#' @param Lambda mat[s, s]: p.d. function. Identity by default
#' @param tol1 double: convergence threshold for manifold convergence
#' @param tol2 double: convergence threhold for gradient descent algorithm. Should be smaller than tol1
#' @param num_iter int: if algorithm does not converge, it iterates for num_iter times.
#' @param ... additional parameter for prior distribution if needed. If not provided, each distribution has its default
#'
#' @return final_points: mat[N, d]: N number of points in R^d, which are points in solution manifold.
#' @export
#'
#' @examples
#' N = 10
#' phi = function(x) {return(pnorm(2, x[[1]], x[[2]]) - pnorm(-5, x[[1]], x[[2]]) - 0.5)}
#' d = 2
#' s = 1
#' res_points = sol_mfd_points(N, phi, d, s, prior = "uniform")
#' head(res_points)
#' phi(res_points[1, ]) # are they on solution manifold?
#' plot(res_points, xlab = "mean", ylab = "sigma", type = 'o') # how they are distributed
sol_mfd_points = function(N, phi, d, s, prior = "gaussian", gamma = 0.005, Lambda = NULL, tol1 = 1e-07, tol2 = 1e-15, num_iter = 100000, ...) {
# construct positive definite function
# compatibility check
if(is.null(Lambda)) {
Lambda = diag(s)
} else {
if(nrow(Lambda) != s || ncol(Lambda) != s) {
stop("Incorrect dimension for Lambda. Lambda must be s x s dim matrix")
}
if(!matrixcalc::is.positive.definite(Lambda)) {
stop("Lambda must be positive definite matrix")
}
}
# result saving matrix
final_points = matrix(0, nrow = N, ncol = d)
iter = 0
# quadratic form and grad of this to run and evaluate algorithm
quadratic_f = pd_function(phi, Lambda)
grad_f = grad_of_quadratic_f(phi, Lambda)
# designate sampling function
sampling_function = sampling_from_dist(d, prior, ...)
final_points = sol_mfd_grad_descent(N, d, quadratic_f, grad_f, sampling_function, gamma, tol1, tol2, num_iter)
return(final_points)
}
#' solving constraint likelihood function using the solution manifold.
#'
#' @param nll function: negative log-likelihood given data X
#' @param C function: constraint function
#' @param theta vector: initial parameter
#' @param s int: output dim of function C
#' @param alpha gradient descent step for nll update
#' @param gamma gradient descent step for solution manifold algorithm
#' @param Lambda positive definite matrix for solution manifold algorithm. Default is identity
#' @param tol1 double: convergence threshold for manifold convergence
#' @param tol2 double: convergence threhold for gradient descent algorithm. Should be smaller than tol1
#' @param num_iter maximum number of iterations for gradient descent.
#' @param num_iter2 number of iteration for all processes.
#'
#' @return theta_traj: matrix containing trajactory of theta updates.
#' @export
#'
#' @examples
#' # init value
#' set.seed(10)
#' # num of samples
#' n = 100
#' # data distribution
#' X = rnorm(n, mean = 1.5, sd = 3)
#' # negative log likelihood
#' nll = function(theta) {return(-sum(dnorm(X, theta[[1]], theta[[2]], log = TRUE)))}
#' # constraint
#' C = function(x) {return(pnorm(2, x[[1]], x[[2]]) - pnorm(-5, x[[1]], x[[2]]) - 0.5)}
#' theta = runif(2, 1, 3)
#' theta_updated = constraint_likelihood(nll, C, theta, 1)
#' # compare between init and end points.
#' C(theta_updated[1, ])
#' C(theta_updated[41, ])
#' nll(theta_updated[1, ])
#' nll(theta_updated[41, ])
constraint_likelihood = function(nll, C, theta, s, alpha = 0.005, gamma = 0.005, Lambda = NULL, tol1 = 1e-07, tol2 = 1e-15, num_iter = 100000, num_iter2 = 20) {
d = length(theta)
# compatibility check
if(is.null(Lambda)) {
Lambda = diag(s)
} else {
if(nrow(Lambda) != s || ncol(Lambda) != s) {
stop("Incorrect dimension for Lambda. Lambda must be s x s dim matrix")
}
if(!matrixcalc::is.positive.definite(Lambda)) {
stop("Lambda must be positive definite matrix")
}
}
grad_nll = grad_of_f(nll)
quadratic_f = pd_function(C, Lambda)
grad_f = grad_of_quadratic_f(C, Lambda)
grad_c = grad_of_f(C)
theta_traj = matrix(NA, 2 * num_iter2 + 1, d)
theta_traj[1, ] = theta
# likelihood update
for(i in 1:num_iter2) {
# gradient descent to negative log likelihood (i.e. gradient ascending to log-likelihood.)
# only one step
theta = theta - alpha * grad_nll(theta)
theta_traj[2 * i, ] = theta
# descent to manifold, until it reaches manifold.
theta = grad_descent(theta, grad_f, d, gamma, tol2, num_iter)
if(quadratic_f(theta) > tol1) {
stop("this theta does not converge to manifold. pick different theta.")
}
theta_traj[2 * i + 1, ] = theta
# stopping criterion: grad_nll(theta) \in span(row(grad_C))
# grad_nll \in row_grad_c
grad_c_mat = grad_c(theta)
# check grad_nll can be approximated by linear projection of rows of grad_c
linear_projection = crossprod(grad_c_mat, solve(tcrossprod(grad_c_mat, grad_c_mat)) %*% grad_c_mat)
target = grad_nll(theta)
row_space_error = sum((target %*% linear_projection - target)^2)
# error < tol1
if (row_space_error < tol2) {
break
}
}
# return with removing NA's
return(theta_traj[!rowSums(!is.finite(theta_traj)),])
}
#' solution manifold points sampling with Posterior density
#'
#' @param X matrix[n, m]. input data. n is sample number and m is dimension of data.
#' @param prob_density function: P(X, Z). It is in fact P(X|Z)
#' @param N int: number of data to ptoduce in solution manifold
#' @param phi function from R^d -> R^s
#' @param d int: input dimension
#' @param s int: output dimension
#' @param k function: kernel function
#' @param h double: normalizing constant. used in kernel. default = 1
#' @param prior str: type of prior distribution. Extra argument ... will be passed to a distribution parameter
#' @param gamma double: parameter of gradient descent step size
#' @param Lambda positive definite function. Default identity
#' @param tol1 double: convergence threshold for manifold convergence
#' @param tol2 double: convergence threhold for gradient descent algorithm. Should be smaller than tol1
#' @param num_iter number of maximum iteration
#' @param ... parameters of prior
#'
#' @return final_points: mat[N, d + 1]: N number of points in R^d+1, which are (R^d dim vector = points in solution manifold, density)
#' @export
#'
#' @examples
#' k = get("dnorm", mode = 'function')
#' N = 10
#' phi = function(x) {return(pnorm(2, x[[1]], x[[2]]) - pnorm(-5, x[[1]], ) - 0.5)}
#' d = 2
#' s = 1
#' prob_density = function(x, theta) {return(dnorm(x, mean = theta[[1]], sd = theta[[2]]))}
#' n = 100
#' set.seed(1)
#' X = rnorm(n, 1.5, 3)
#' res_with_density = post_density_solmfd(X, prob_density, N, phi, d, s, k, prior = "uniform", gamma = 0.1, min = 0, max = 1)
#' head(res_with_density)
post_density_solmfd = function(X, prob_density, N, phi, d, s, k, h = 1, prior = "gaussian", gamma = 0.01, Lambda = NULL, tol1 = 1e-07, tol2 = 1e-15, num_iter = 100000, ...) {
# obtain points
points = sol_mfd_points(N, phi, d, s, prior, gamma, Lambda, tol1, tol2, num_iter, ...)
# kernel
dist_between_row = between_row_dist(points, points)
normalized = dist_between_row / h
kerneled = k(normalized)
# mean by N
rho_i = rowMeans(kerneled)
# calculate the \hat(pi_i) = \pi(Z_i) \prod(p(X_j | Z+i))
# calculate pi_i
# prior setting
# designate sampling function
density_function = density_from_dist(d, prior, ...)
pi_z = density_function(points)
# conditioning X ftn for vectoization
z_conditioning_x = function(z) { return(prob_density(X, z)) }
pi_i = exp(log(pi_z) + colSums(log(apply(points, 1, z_conditioning_x))))
omega_i = pi_i / rho_i
return(cbind(points, omega_i))
}
N = 10
phi = function(x) {return(pnorm(2, x[[1]], x[[2]]) - pnorm(-5, x[[1]], x[[2]]) - 0.5)}
d = 2
s = 1
res_points = sol_mfd_points(N, phi, d, s, prior = "uniform")
head(res_points)
phi(res_points[1, ]) # are they on solution manifold?
plot(res_points, xlab = "mean", ylab = "sigma", type = 'o') # how they are distributed
set.seed(10)
N = 10
phi = function(x) {return(pnorm(2, x[[1]], x[[2]]) - pnorm(-5, x[[1]], x[[2]]) - 0.5)}
d = 2
s = 1
res_points = sol_mfd_points(N, phi, d, s, prior = "uniform")
head(res_points)
phi(res_points[1, ]) # are they on solution manifold?
plot(res_points, xlab = "mean", ylab = "sigma", type = 'o') # how they are distributed
plot(res_points, xlab = "mean", ylab = "sigma") # how they are distributed
res_points = sol_mfd_points(N, phi, d, s, prior = "uniform", min = 0, max = 10)
# num of samples
n = 100
# data distribution
X = rnorm(n, mean = 1.5, sd = 3)
# negative log likelihood
nll = function(theta) {return(-sum(dnorm(X, theta[[1]], theta[[2]], log = TRUE)))}
# constraint
C = function(x) {return(pnorm(2, x[[1]], x[[2]]) - pnorm(-5, x[[1]], x[[2]]) - 0.5)}
theta = runif(2, 1, 3)
theta_updated = constraint_likelihood(nll, C, theta, 1)
# compare between init and end points.
plot(x = seq(1, nrow(theta_updated)), apply(theta_updated, 1, C), xlab = "step", ylab = "constraint", type = 'o')
C(theta_updated[1, ])
C(theta_updated[41, ])
plot(x = seq(1, nrow(theta_updated)), apply(theta_updated, 1, nll), xlab = "step", ylab = "constraint", type = 'o')
plot(theta_updated, xlab = "mean", ylab = "sigma", type = 'o')
lines(theta_updated[seq(3, nrow(theta_updated), by = 2), ],  col = 'red')
k = get("dnorm", mode = 'function')
N = 10
phi = function(x) {return(pnorm(2, x[[1]], x[[2]]) - pnorm(-5, x[[1]], ) - 0.5)}
d = 2
s = 1
prob_density = function(x, theta) {return(dnorm(x, mean = theta[[1]], sd = theta[[2]]))}
n = 100
set.seed(1)
set.seed(10)
n = 100
X = rnorm(n, 1.5, 3)
res_with_density = post_density_solmfd(X, prob_density, N, phi, d, s, k, prior = "uniform", gamma = 0.1, min = 0, max = 1)
head(res_with_density)
res_with_density = post_density_solmfd(X, prob_density, N, phi, d, s, k, prior = "uniform", gamma = 0.1, min = 0, max = 10)
head(res_with_density)
plot(res_with_density)
plot(res_with_density[, 3])
library(SolMfd)
devtools::install_github("wldyddl5510/SolMfd")
remove.packages(SolMfd)
remove.packages("SolMfd")
devtools::install_github("wldyddl5510/SolMfd")
devtools::install_github("wldyddl5510/SolMfd")
detach
detach("package:SolMfd")
devtools::install_github("wldyddl5510/SolMfd")
N = 10
#'
phi = function(x) {return(pnorm(2, x[[1]], x[[2]]) - pnorm(-5, x[[1]], x[[2]]) - 0.5)}
d =2
s = 1
res_points = sol_mfd_points(N, phi, d, s, prior = "uniform")
detach("package:SolMfd")
remove.packages("SolMfd")
detach("package:SolMfd")
detach("packages:SolMfd")
detach("SolMfd")
devtools::install_github("wldyddl5510/SolMfd")
