# negative log likelihood
nll = function(theta) {return(-sum(dnorm(X, theta[[1]], theta[[2]], log = TRUE)))}
# constraint
C = function(x) {return(pnorm(2, x[[1]], x[[2]]) - pnorm(-5, x[[1]], x[[2]]) - 0.5)}
theta = runif(2, 1, 3)
theta_updated = constraint_likelihood(nll, C, theta, 1)
#' nll = function(theta) {return(-sum(dnorm(X, theta[[1]], theta[[2]], log = TRUE)))}
#' # constraint
#' C = function(x) {return(pnorm(2, x[[1]], x[[2]]) - pnorm(-5, x[[1]], x[[2]]) - 0.5)}
#' theta = runif(2, 1, 3)
#' theta_updated = constraint_likelihood(nll, C, theta, 1)
#' # compare two points.
#' C(theta)
#' C(theta_updated)
#' nll(theta)
#' nll(theta_updated)
constraint_likelihood = function(nll, C, theta, s, alpha = 0.01, gamma = 0.01, Lambda = NULL, tol1 = 1e-07, tol2 = 1e-15, num_iter = 100000) {
d = length(theta)
# compatibility check
if(is.null(Lambda)) {
Lambda = diag(s)
} else {
if(nrow(Lambda) != s || ncol(Lambda) != s) {
stop("Incorrect dimension for Lambda. Lambda must be s x s dim matrix")
}
if(!matrixcalc::is.positive.definite(Lambda)) {
stop("Lambda must be positive definite matrix")
}
}
grad_nll = grad_of_f(nll)
quadratic_f = pd_function(C, Lambda)
grad_f = grad_of_quadratic_f(C, Lambda)
grad_c = grad_of_f(C)
# likelihood update
for(i in 1:num_iter) {
print(i)
# gradient descent to negative log likelihood (i.e. gradient ascending to log-likelihood.)
theta = grad_descent(theta, grad_nll, d, alpha, tol2, num_iter)
# descent to manifold, until it reaches manifold.
theta = grad_descent(theta, grad_f, d, gamma, tol2, num_iter)
if(quadratic_f(theta) > tol1) {
stop("this theta does not converge to manifold. pick different theta.")
}
# stopping criterion: grad_nll(theta) \in span(row(grad_C))
grad_c_mat = grad_c(theta)
# check grad_nll can be approximated by linear projection of rows of grad_c
linear_projection = crossprod(grad_c_mat, solve(tcrossprod(grad_c_mat, grad_c_mat)) %*% grad_c_mat)
target = grad_nll(theta)
row_space_error = sum((target %*% linear_projection - target)^2)
print(row_space_error)
# error < tol1
if (row_space_error < tol2) {
break
}
}
return(theta)
}
# init value
set.seed(1)
# num of samples
n = 100
# data distribution
X = rnorm(n, mean = 1.5, sd = 3)
# negative log likelihood
nll = function(theta) {return(-sum(dnorm(X, theta[[1]], theta[[2]], log = TRUE)))}
# constraint
C = function(x) {return(pnorm(2, x[[1]], x[[2]]) - pnorm(-5, x[[1]], x[[2]]) - 0.5)}
theta = runif(2, 1, 3)
theta_updated = constraint_likelihood(nll, C, theta, 1)
#' nll = function(theta) {return(-sum(dnorm(X, theta[[1]], theta[[2]], log = TRUE)))}
#' # constraint
#' C = function(x) {return(pnorm(2, x[[1]], x[[2]]) - pnorm(-5, x[[1]], x[[2]]) - 0.5)}
#' theta = runif(2, 1, 3)
#' theta_updated = constraint_likelihood(nll, C, theta, 1)
#' # compare two points.
#' C(theta)
#' C(theta_updated)
#' nll(theta)
#' nll(theta_updated)
constraint_likelihood = function(nll, C, theta, s, alpha = 0.01, gamma = 0.01, Lambda = NULL, tol1 = 1e-07, tol2 = 1e-15, num_iter = 100000, num_iter2 = 50) {
d = length(theta)
# compatibility check
if(is.null(Lambda)) {
Lambda = diag(s)
} else {
if(nrow(Lambda) != s || ncol(Lambda) != s) {
stop("Incorrect dimension for Lambda. Lambda must be s x s dim matrix")
}
if(!matrixcalc::is.positive.definite(Lambda)) {
stop("Lambda must be positive definite matrix")
}
}
grad_nll = grad_of_f(nll)
quadratic_f = pd_function(C, Lambda)
grad_f = grad_of_quadratic_f(C, Lambda)
grad_c = grad_of_f(C)
# likelihood update
for(i in 1:num_iter2) {
print(i)
# gradient descent to negative log likelihood (i.e. gradient ascending to log-likelihood.)
theta = grad_descent(theta, grad_nll, d, alpha, tol2, num_iter)
# descent to manifold, until it reaches manifold.
theta = grad_descent(theta, grad_f, d, gamma, tol2, num_iter)
if(quadratic_f(theta) > tol1) {
stop("this theta does not converge to manifold. pick different theta.")
}
# stopping criterion: grad_nll(theta) \in span(row(grad_C))
grad_c_mat = grad_c(theta)
# check grad_nll can be approximated by linear projection of rows of grad_c
linear_projection = crossprod(grad_c_mat, solve(tcrossprod(grad_c_mat, grad_c_mat)) %*% grad_c_mat)
target = grad_nll(theta)
row_space_error = sum((target %*% linear_projection - target)^2)
print(nll(theta))
print(quadratic_f(theta))
# error < tol1
if (row_space_error < tol2) {
break
}
}
return(theta)
}
theta = runif(2, 1, 3)
theta_updated = constraint_likelihood(nll, C, theta, 1)
#' nll = function(theta) {return(-sum(dnorm(X, theta[[1]], theta[[2]], log = TRUE)))}
#' # constraint
#' C = function(x) {return(pnorm(2, x[[1]], x[[2]]) - pnorm(-5, x[[1]], x[[2]]) - 0.5)}
#' theta = runif(2, 1, 3)
#' theta_updated = constraint_likelihood(nll, C, theta, 1)
#' # compare two points.
#' C(theta)
#' C(theta_updated)
#' nll(theta)
#' nll(theta_updated)
constraint_likelihood = function(nll, C, theta, s, alpha = 0.01, gamma = 0.01, Lambda = NULL, tol1 = 1e-07, tol2 = 1e-15, num_iter = 100000, num_iter2 = 50) {
d = length(theta)
# compatibility check
if(is.null(Lambda)) {
Lambda = diag(s)
} else {
if(nrow(Lambda) != s || ncol(Lambda) != s) {
stop("Incorrect dimension for Lambda. Lambda must be s x s dim matrix")
}
if(!matrixcalc::is.positive.definite(Lambda)) {
stop("Lambda must be positive definite matrix")
}
}
grad_nll = grad_of_f(nll)
quadratic_f = pd_function(C, Lambda)
grad_f = grad_of_quadratic_f(C, Lambda)
grad_c = grad_of_f(C)
# likelihood update
for(i in 1:num_iter2) {
print(i)
theta_old = theta
# gradient descent to negative log likelihood (i.e. gradient ascending to log-likelihood.)
theta = grad_descent(theta, grad_nll, d, alpha, tol2, num_iter)
print(theta)
print(nll(theta))
print(quadratic_f(theta))
# descent to manifold, until it reaches manifold.
theta = grad_descent(theta, grad_f, d, gamma, tol2, num_iter)
if(quadratic_f(theta) > tol1) {
stop("this theta does not converge to manifold. pick different theta.")
}
print(theta)
print(nll(theta))
print(quadratic_f(theta))
# stopping criterion: grad_nll(theta) \in span(row(grad_C)) or conv of theta
# if(sum((theta - theta_old)^2) < tol2) {
#   break
# }
# grad_nll \in row_grad_c
grad_c_mat = grad_c(theta)
# check grad_nll can be approximated by linear projection of rows of grad_c
linear_projection = crossprod(grad_c_mat, solve(tcrossprod(grad_c_mat, grad_c_mat)) %*% grad_c_mat)
target = grad_nll(theta)
row_space_error = sum((target %*% linear_projection - target)^2)
# error < tol1
if (row_space_error < tol2) {
break
}
}
return(theta)
}
theta = runif(2, 1, 3)
theta_updated = constraint_likelihood(nll, C, theta, 1)
# compare two points.
C(theta)
C(theta_updated)
nll(theta)
nll(theta_updated)
rep(c(0,0), 10)
rep(c(0,1), 10)
rep(c(0,1), 10)[3]
matrix(NA, 2, 2)
matrix(NA, 2, 2)[1, ] = c(1, 0)
matrix(NaN, 2, 2)[1, ] = c(1, 0)
matrix(NaN, 2, 2)[1, ]
X = matrix(NA, 2, 2)
X[1, ] = c(0, 1)
X
X = matrix(NA, 2, 2, na.rm = TRUE)
X = matrix(NA, 2, 2)
X[!is.na(X)]
X[1, ] = c(0, 1)
X[!is.na(X)]
X = matrix(NA, 3, 2)
X[1, ] = c(0, 1)
X[3, ] = c(0, 1)
X[!is.na(X)]
X[!rowSums(!is.finite(X)),]
constraint_likelihood = function(nll, C, theta, s, alpha = 0.01, gamma = 0.01, Lambda = NULL, tol1 = 1e-07, tol2 = 1e-15, num_iter = 100000, num_iter2 = 50) {
d = length(theta)
# compatibility check
if(is.null(Lambda)) {
Lambda = diag(s)
} else {
if(nrow(Lambda) != s || ncol(Lambda) != s) {
stop("Incorrect dimension for Lambda. Lambda must be s x s dim matrix")
}
if(!matrixcalc::is.positive.definite(Lambda)) {
stop("Lambda must be positive definite matrix")
}
}
grad_nll = grad_of_f(nll)
quadratic_f = pd_function(C, Lambda)
grad_f = grad_of_quadratic_f(C, Lambda)
grad_c = grad_of_f(C)
theta_traj = matrix(NA, num_iter2, d)
# likelihood update
for(i in 1:num_iter2) {
# gradient descent to negative log likelihood (i.e. gradient ascending to log-likelihood.)
theta = grad_descent(theta, grad_nll, d, alpha, tol2, num_iter)
# descent to manifold, until it reaches manifold.
theta = grad_descent(theta, grad_f, d, gamma, tol2, num_iter)
if(quadratic_f(theta) > tol1) {
stop("this theta does not converge to manifold. pick different theta.")
}
theta_traj[i, ] = theta
# stopping criterion: grad_nll(theta) \in span(row(grad_C))
# grad_nll \in row_grad_c
grad_c_mat = grad_c(theta)
# check grad_nll can be approximated by linear projection of rows of grad_c
linear_projection = crossprod(grad_c_mat, solve(tcrossprod(grad_c_mat, grad_c_mat)) %*% grad_c_mat)
target = grad_nll(theta)
row_space_error = sum((target %*% linear_projection - target)^2)
# error < tol1
if (row_space_error < tol2) {
break
}
}
# return with removing NA's
return(theta_traj[!rowSums(!is.finite(theta_traj)),])
}
#' nll = function(theta) {return(-sum(dnorm(X, theta[[1]], theta[[2]], log = TRUE)))}
#' # constraint
#' C = function(x) {return(pnorm(2, x[[1]], x[[2]]) - pnorm(-5, x[[1]], x[[2]]) - 0.5)}
#' theta = runif(2, 1, 3)
#' theta_updated = constraint_likelihood(nll, C, theta, 1)
#' # compare two points.
#' C(theta)
#' C(theta_updated)
#' nll(theta)
#' nll(theta_updated)
constraint_likelihood = function(nll, C, theta, s, alpha = 0.01, gamma = 0.01, Lambda = NULL, tol1 = 1e-07, tol2 = 1e-15, num_iter = 100000, num_iter2 = 20) {
d = length(theta)
# compatibility check
if(is.null(Lambda)) {
Lambda = diag(s)
} else {
if(nrow(Lambda) != s || ncol(Lambda) != s) {
stop("Incorrect dimension for Lambda. Lambda must be s x s dim matrix")
}
if(!matrixcalc::is.positive.definite(Lambda)) {
stop("Lambda must be positive definite matrix")
}
}
grad_nll = grad_of_f(nll)
quadratic_f = pd_function(C, Lambda)
grad_f = grad_of_quadratic_f(C, Lambda)
grad_c = grad_of_f(C)
theta_traj = matrix(NA, num_iter2, d)
# likelihood update
for(i in 1:num_iter2) {
# gradient descent to negative log likelihood (i.e. gradient ascending to log-likelihood.)
theta = grad_descent(theta, grad_nll, d, alpha, tol2, num_iter)
# descent to manifold, until it reaches manifold.
theta = grad_descent(theta, grad_f, d, gamma, tol2, num_iter)
if(quadratic_f(theta) > tol1) {
stop("this theta does not converge to manifold. pick different theta.")
}
theta_traj[i, ] = theta
# stopping criterion: grad_nll(theta) \in span(row(grad_C))
# grad_nll \in row_grad_c
grad_c_mat = grad_c(theta)
# check grad_nll can be approximated by linear projection of rows of grad_c
linear_projection = crossprod(grad_c_mat, solve(tcrossprod(grad_c_mat, grad_c_mat)) %*% grad_c_mat)
target = grad_nll(theta)
row_space_error = sum((target %*% linear_projection - target)^2)
# error < tol1
if (row_space_error < tol2) {
break
}
}
# return with removing NA's
return(theta_traj[!rowSums(!is.finite(theta_traj)),])
}
# init value
set.seed(1)
# num of samples
n = 100
# data distribution
X = rnorm(n, mean = 1.5, sd = 3)
# negative log likelihood
nll = function(theta) {return(-sum(dnorm(X, theta[[1]], theta[[2]], log = TRUE)))}
# constraint
C = function(x) {return(pnorm(2, x[[1]], x[[2]]) - pnorm(-5, x[[1]], x[[2]]) - 0.5)}
theta = runif(2, 1, 3)
theta_updated = constraint_likelihood(nll, C, theta, 1)
theta_updated
theta = runif(2, 1, 3)
theta
theta_updated = constraint_likelihood(nll, C, theta, 1)
theta_updated
#' nll = function(theta) {return(-sum(dnorm(X, theta[[1]], theta[[2]], log = TRUE)))}
#' # constraint
#' C = function(x) {return(pnorm(2, x[[1]], x[[2]]) - pnorm(-5, x[[1]], x[[2]]) - 0.5)}
#' theta = runif(2, 1, 3)
#' theta_updated = constraint_likelihood(nll, C, theta, 1)
#' # compare two points.
#' C(theta)
#' C(theta_updated)
#' nll(theta)
#' nll(theta_updated)
constraint_likelihood = function(nll, C, theta, s, alpha = 0.01, gamma = 0.01, Lambda = NULL, tol1 = 1e-07, tol2 = 1e-15, num_iter = 100000, num_iter2 = 20) {
d = length(theta)
# compatibility check
if(is.null(Lambda)) {
Lambda = diag(s)
} else {
if(nrow(Lambda) != s || ncol(Lambda) != s) {
stop("Incorrect dimension for Lambda. Lambda must be s x s dim matrix")
}
if(!matrixcalc::is.positive.definite(Lambda)) {
stop("Lambda must be positive definite matrix")
}
}
grad_nll = grad_of_f(nll)
quadratic_f = pd_function(C, Lambda)
grad_f = grad_of_quadratic_f(C, Lambda)
grad_c = grad_of_f(C)
theta_traj = matrix(NA, 2 * num_iter2 + 1, d)
theta_traj[1, ] = theta
# likelihood update
for(i in 1:num_iter2) {
# gradient descent to negative log likelihood (i.e. gradient ascending to log-likelihood.)
theta = grad_descent(theta, grad_nll, d, alpha, tol2, num_iter)
theta_traj[2 * i, ] = theta
# descent to manifold, until it reaches manifold.
theta = grad_descent(theta, grad_f, d, gamma, tol2, num_iter)
if(quadratic_f(theta) > tol1) {
stop("this theta does not converge to manifold. pick different theta.")
}
theta_traj[2 * i + 1, ] = theta
# stopping criterion: grad_nll(theta) \in span(row(grad_C))
# grad_nll \in row_grad_c
grad_c_mat = grad_c(theta)
# check grad_nll can be approximated by linear projection of rows of grad_c
linear_projection = crossprod(grad_c_mat, solve(tcrossprod(grad_c_mat, grad_c_mat)) %*% grad_c_mat)
target = grad_nll(theta)
row_space_error = sum((target %*% linear_projection - target)^2)
# error < tol1
if (row_space_error < tol2) {
break
}
}
# return with removing NA's
return(theta_traj[!rowSums(!is.finite(theta_traj)),])
}
theta = runif(2, 1, 3)
theta_updated = constraint_likelihood(nll, C, theta, 1)
theta_updated
# init value
set.seed(100)
# num of samples
n = 100
# data distribution
X = rnorm(n, mean = 1.5, sd = 3)
# negative log likelihood
nll = function(theta) {return(-sum(dnorm(X, theta[[1]], theta[[2]], log = TRUE)))}
# constraint
C = function(x) {return(pnorm(2, x[[1]], x[[2]]) - pnorm(-5, x[[1]], x[[2]]) - 0.5)}
theta = runif(2, 1, 3)
theta_updated = constraint_likelihood(nll, C, theta, 1)
theta_updated
theta_updated = constraint_likelihood(nll, C, theta, 1, alpha = 0.1, gamma = 0.05)
theta_updated = constraint_likelihood(nll, C, theta, 1, alpha = 0.1, gamma = 0.01)
# compare two points.
C(theta)
theta_updated = constraint_likelihood(nll, C, theta, 1, alpha = 0.05, gamma = 0.01)
theta_updated
#' nll = function(theta) {return(-sum(dnorm(X, theta[[1]], theta[[2]], log = TRUE)))}
#' # constraint
#' C = function(x) {return(pnorm(2, x[[1]], x[[2]]) - pnorm(-5, x[[1]], x[[2]]) - 0.5)}
#' theta = runif(2, 1, 3)
#' theta_updated = constraint_likelihood(nll, C, theta, 1)
#' # compare two points.
#' C(theta)
#' C(theta_updated)
#' nll(theta)
#' nll(theta_updated)
constraint_likelihood = function(nll, C, theta, s, alpha = 0.01, gamma = 0.01, Lambda = NULL, tol1 = 1e-07, tol2 = 1e-15, num_iter = 100000, num_iter2 = 20) {
d = length(theta)
# compatibility check
if(is.null(Lambda)) {
Lambda = diag(s)
} else {
if(nrow(Lambda) != s || ncol(Lambda) != s) {
stop("Incorrect dimension for Lambda. Lambda must be s x s dim matrix")
}
if(!matrixcalc::is.positive.definite(Lambda)) {
stop("Lambda must be positive definite matrix")
}
}
grad_nll = grad_of_f(nll)
quadratic_f = pd_function(C, Lambda)
grad_f = grad_of_quadratic_f(C, Lambda)
grad_c = grad_of_f(C)
theta_traj = matrix(NA, 2 * num_iter2 + 1, d)
theta_traj[1, ] = theta
# likelihood update
for(i in 1:num_iter2) {
# gradient descent to negative log likelihood (i.e. gradient ascending to log-likelihood.)
# theta = grad_descent(theta, grad_nll, d, alpha, tol2, num_iter)
theta = theta - alpha * grad_nll(theta)
theta_traj[2 * i, ] = theta
# descent to manifold, until it reaches manifold.
theta = grad_descent(theta, grad_f, d, gamma, tol2, num_iter)
if(quadratic_f(theta) > tol1) {
stop("this theta does not converge to manifold. pick different theta.")
}
theta_traj[2 * i + 1, ] = theta
# stopping criterion: grad_nll(theta) \in span(row(grad_C))
# grad_nll \in row_grad_c
grad_c_mat = grad_c(theta)
# check grad_nll can be approximated by linear projection of rows of grad_c
linear_projection = crossprod(grad_c_mat, solve(tcrossprod(grad_c_mat, grad_c_mat)) %*% grad_c_mat)
target = grad_nll(theta)
row_space_error = sum((target %*% linear_projection - target)^2)
# error < tol1
if (row_space_error < tol2) {
break
}
}
# return with removing NA's
return(theta_traj[!rowSums(!is.finite(theta_traj)),])
}
theta = runif(2, 1, 3)
theta_updated = constraint_likelihood(nll, C, theta, 1, alpha = 0.05, gamma = 0.01)
theta_updated
theta_updated = constraint_likelihood(nll, C, theta, 1, alpha = 0.05, gamma = 0.01, tol2 = 1e-13)
theta_updated
plot(theta_traj)
plot(theta_updated)
theta = runif(2, 1, 3)
theta_updated = constraint_likelihood(nll, C, theta, 1, alpha = 0.001, gamma = 0.001, tol2 = 1e-13)
# init value
set.seed(10)
# num of samples
n = 100
# data distribution
X = rnorm(n, mean = 1.5, sd = 3)
# negative log likelihood
nll = function(theta) {return(-sum(dnorm(X, theta[[1]], theta[[2]], log = TRUE)))}
# constraint
C = function(x) {return(pnorm(2, x[[1]], x[[2]]) - pnorm(-5, x[[1]], x[[2]]) - 0.5)}
theta = runif(2, 1, 3)
theta_updated = constraint_likelihood(nll, C, theta, 1, alpha = 0.001, gamma = 0.001, tol2 = 1e-13)
theta_updated = constraint_likelihood(nll, C, theta, 1, alpha = 0.001, gamma = 0.01, tol2 = 1e-13)
theta_updated
plot(theta_updated)
theta_updated = constraint_likelihood(nll, C, theta, 1, alpha = 0.001, gamma = 0.01, tol2 = 1e-15)
theta_updated
plot(theta_updated)
theta_updated = constraint_likelihood(nll, C, theta, 1, alpha = 0.005, gamma = 0.005, tol2 = 1e-15)
theta_updated
plot(theta_updated)
# compare two points.
C(theta_traj[41, ])
# compare two points.
C(theta_updated[41, ])
# compare two points.
C(theta_updated[40, ])
# compare two points.
C(theta_updated[40, ])
C(theta_updated[41, ])
nll(theta[40, ])
nll(theta_updated[41, ])
nll(theta_updated[40, ])
nll(theta_updated[41, ])
# compare two points.
C(theta_updated[1, ])
C(theta_updated[41, ])
nll(theta_updated[1, ])
nll(theta_updated[41, ])
plot(theta_updated[seq(1, 41, by = 2)])
plot(theta_updated[seq(2, 40, by = 2)])
plot(theta_updated[seq(1, 41, by = 2)], )
plot(theta_updated[seq(2, 40, by = 2)], )
plot(theta_updated[seq(2, 40, by = 2), ] )
plot(theta_updated[seq(1, 41, by = 2), ] )
roxygenise(clean=TRUE)
